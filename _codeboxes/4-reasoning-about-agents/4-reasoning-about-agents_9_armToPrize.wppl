var armToPrize = {0: 'chocolate',
		          1: 'champagne'};
var worldAndStart = makeIRLBanditWorldAndStart(2, armToPrize, 5);
var observe = worldAndStart.world.observe;
var fullObserve = getFullObserve(observe);
var transition = worldAndStart.world.transition;

var makeTrajectory = function(state) {
  var observation = fullObserve(state);
  var action = 1; // agent always pulls arm 1
  var nextState = transition(state, action);
  var out = {state: state,
	         observation: observation,
	         action: action};
  if (state.manifestState.terminateAfterAction) {
    return out;
  } else {
    return cons(out, makeTrajectory(nextState));
  }
};

var observedSequence = makeTrajectory(worldAndStart.startState);

var baseParams = {
  alpha: 100
};

var agentPrior = Enumerate(function(){
  var latent = flip(0.5) ? armToPrize : update(armToPrize, {1: 'nothing'});
  return buildState(worldAndStart.startState.manifestState, latent);
});
var priorInitialBelief = deltaERP(agentPrior);

var likesChampagne = {nothing: 0,
		              champagne: 5,
					  chocolate: 3};
var likesChocolate = {nothing: 0,
		              champagne: 3,
					  chocolate: 5};

var priorPrizeToUtility = categoricalERP([0.5, 0.5], [likesChampagne,
						                              likesChocolate]);

var posterior = agentModelsIRLBanditInfer(baseParams, priorPrizeToUtility,
					                      priorInitialBelief, worldAndStart,
										  observedSequence);

var chocolateUtilityPosterior = Enumerate(function(){
  var utilityBelief = sample(posterior);
  var likesChocolate = utilityBelief.prizeToUtility.chocolate > 3;
  return {likesChocolate: likesChocolate};
});
  
viz.vegaPrint(chocolateUtilityPosterior);
